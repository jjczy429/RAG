"""
RAGå†…å®¹å®‰å…¨å®¡æ ¸ç³»ç»Ÿ - åŸºäºé¢†åŸŸçŸ¥è¯†åº“çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
ä¾èµ–å®‰è£…ï¼š
pip install streamlit pandas numpy sentence-transformers openai zhipuai dashscope python-dotenv
"""

import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
from sentence_transformers import SentenceTransformer
from datetime import datetime
import json
from openai import OpenAI
import dashscope
from zhipuai import ZhipuAI
from typing import List, Dict, Any, Optional, Tuple
import heapq


# ==================== é…ç½®éƒ¨åˆ† ====================
class Config:
    VECTOR_DB_PATH = "vector_db.npz"  # ä¿®æ”¹ä¸ºnumpyæ ¼å¼
    METADATA_PATH = "metadata.pkl"
    EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"
    VECTOR_DIM = 384
    TOP_K = 5
    RERANK_TOP_K = 3

    # LLMé…ç½®
    SUPPORTED_MODELS = {
        "OpenAI GPT-4": {"provider": "openai", "model": "gpt-4"},
        "OpenAI GPT-3.5": {"provider": "openai", "model": "gpt-3.5-turbo"},
        "é€šä¹‰åƒé—® Qwen-Plus": {"provider": "dashscope", "model": "qwen-plus"},
        "é€šä¹‰åƒé—® Qwen-Turbo": {"provider": "dashscope", "model": "qwen-turbo"},
        "æ™ºè°± GLM-4-FLASH": {"provider": "zhipu", "model": "glm-4-flash"},
        "æ™ºè°± GLM-3-Turbo": {"provider": "zhipu", "model": "glm-3-turbo"},
    }


# ==================== çº¯numpyå®ç°çš„å‘é‡ç´¢å¼• ====================
class NumpyVectorIndex:
    """çº¯numpyå®ç°çš„å‘é‡ç´¢å¼•ï¼Œæ›¿ä»£faiss"""
    
    def __init__(self, dimension: int):
        self.dimension = dimension
        self.vectors = np.zeros((0, dimension), dtype=np.float32)
        self.normalized = False
    
    def add(self, vectors: np.ndarray):
        """æ·»åŠ å‘é‡åˆ°ç´¢å¼•"""
        if len(vectors.shape) != 2 or vectors.shape[1] != self.dimension:
            raise ValueError(f"Expected 2D array of shape (n, {self.dimension}), got {vectors.shape}")
            
        # å¦‚æœå½“å‰æ²¡æœ‰å‘é‡ï¼Œç›´æ¥æ·»åŠ 
        if self.vectors.size == 0:
            self.vectors = vectors.astype(np.float32)
        else:
            self.vectors = np.vstack([self.vectors, vectors.astype(np.float32)])
    
    def search(self, query: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        æœç´¢æœ€ç›¸ä¼¼çš„kä¸ªå‘é‡
        è¿”å›: (distances, indices)
        """
        if self.vectors.size == 0:
            return np.array([]), np.array([])
            
        query = query.astype(np.float32)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (å†…ç§¯ï¼Œå› ä¸ºå‘é‡å·²ç»å½’ä¸€åŒ–)
        similarities = np.dot(query, self.vectors.T)  # shape: (1, n_vectors)
        
        # è·å–top-k
        if k >= len(self.vectors):
            k = len(self.vectors)
        
        # ä½¿ç”¨å †æ¥è·å–top-k
        if k <= 0:
            return np.empty((1, 0), dtype=np.float32), np.empty((1, 0), dtype=np.int64)

        # np.argpartition çš„ kth æ˜¯ 0-basedï¼Œå¿…é¡» < n
        n = similarities.shape[1]
        if k > n:
            k = n

        top_k_indices = np.argpartition(-similarities[0], k - 1)[:k]
        top_k_scores = similarities[0][top_k_indices]
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_indices = np.argsort(-top_k_scores)
        return top_k_scores[sorted_indices].reshape(1, -1), top_k_indices[sorted_indices].reshape(1, -1)
    
    def __len__(self) -> int:
        """è¿”å›å‘é‡æ•°é‡"""
        return len(self.vectors)
    
    @property
    def ntotal(self) -> int:
        """å…¼å®¹faissæ¥å£ï¼Œè¿”å›å‘é‡æ•°é‡"""
        return len(self)
    
    def save(self, filepath: str):
        """ä¿å­˜å‘é‡åˆ°æ–‡ä»¶"""
        np.savez_compressed(filepath, vectors=self.vectors)
    
    @classmethod
    def load(cls, filepath: str, dimension: int) -> 'NumpyVectorIndex':
        """ä»æ–‡ä»¶åŠ è½½å‘é‡"""
        if not os.path.exists(filepath):
            return cls(dimension)
            
        data = np.load(filepath)
        index = cls(dimension)
        index.vectors = data['vectors']
        return index


# ==================== LLM APIè°ƒç”¨ç±» ====================
class LLMClient:
    def __init__(self, provider, model, api_key):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        self.provider = provider
        self.model = model
        self.api_key = api_key

        if provider == "openai":
            self.client = OpenAI(api_key=api_key)
        elif provider == "dashscope":
            dashscope.api_key = api_key
        elif provider == "zhipu":
            self.client = ZhipuAI(api_key=api_key)

    def generate(self, prompt, temperature=0.7, max_tokens=2000):
        """è°ƒç”¨LLMç”Ÿæˆå›ç­”"""
        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹å®‰å…¨å®¡æ ¸åŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return response.choices[0].message.content

            elif self.provider == "dashscope":
                from dashscope import Generation
                response = Generation.call(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹å®‰å…¨å®¡æ ¸åŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    result_format='message'
                )
                if response.status_code == 200:
                    return response.output.choices[0].message.content
                else:
                    return f"APIè°ƒç”¨å¤±è´¥: {response.message}"

            elif self.provider == "zhipu":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹å®‰å…¨å®¡æ ¸åŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return response.choices[0].message.content

        except Exception as e:
            return f"âŒ LLMè°ƒç”¨é”™è¯¯: {str(e)}\n\nè¯·æ£€æŸ¥:\n1. APIå¯†é’¥æ˜¯å¦æ­£ç¡®\n2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n3. APIé¢åº¦æ˜¯å¦å……è¶³"


# ==================== å‘é‡æ•°æ®åº“ç®¡ç†ç±» ====================
class VectorDatabase:
    def __init__(self, model_name=Config.EMBEDDING_MODEL):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        self.model = SentenceTransformer(model_name)
        self.dimension = Config.VECTOR_DIM
        self.index = NumpyVectorIndex(self.dimension)  # ä½¿ç”¨çº¯numpyå®ç°
        self.metadata = []
        self.load_or_create_index()

    def load_or_create_index(self):
        """åŠ è½½æˆ–åˆ›å»ºç´¢å¼•"""
        if os.path.exists(Config.VECTOR_DB_PATH) and os.path.exists(Config.METADATA_PATH):
            self.index = NumpyVectorIndex.load(Config.VECTOR_DB_PATH, self.dimension)
            with open(Config.METADATA_PATH, 'rb') as f:
                self.metadata = pickle.load(f)
        else:
            self.index = NumpyVectorIndex(self.dimension)
            self.metadata = []

    def save_index(self):
        """ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®"""
        self.index.save(Config.VECTOR_DB_PATH)
        with open(Config.METADATA_PATH, 'wb') as f:
            pickle.dump(self.metadata, f)

    def build_from_csv(self, csv_path, text_column, category_column=None):
        """ä»CSVæ–‡ä»¶æ„å»ºå‘é‡æ•°æ®åº“"""
        # Streamlit ä¸Šä¼ ç»„ä»¶è¿”å›çš„æ˜¯ UploadedFileï¼›è¿™é‡Œå…¼å®¹ UploadedFile / æ–‡ä»¶è·¯å¾„ / ç±»æ–‡ä»¶å¯¹è±¡
        try:
            if hasattr(csv_path, "getvalue"):
                raw = csv_path.getvalue()
                if not raw or not raw.strip():
                    raise ValueError("ä¸Šä¼ çš„CSVä¸ºç©ºï¼ˆæ–‡ä»¶å¤§å°ä¸º0æˆ–åªæœ‰ç©ºç™½å†…å®¹ï¼‰ã€‚è¯·æ£€æŸ¥å¯¼å‡ºçš„CSVæ˜¯å¦åŒ…å«è¡¨å¤´å’Œæ•°æ®ã€‚")
                from io import BytesIO
                bio = BytesIO(raw)
                # ä¼˜å…ˆå°è¯• utf-8-sigï¼Œå…¶æ¬¡ gbk
                try:
                    df = pd.read_csv(bio, encoding="utf-8-sig")
                except Exception:
                    bio.seek(0)
                    df = pd.read_csv(bio, encoding="gbk")
            else:
                df = pd.read_csv(csv_path)
        except pd.errors.EmptyDataError:
            raise ValueError("æ— æ³•è§£æCSVï¼šæ²¡æœ‰è¯»åˆ°ä»»ä½•åˆ—ï¼ˆNo columns to parseï¼‰ã€‚è¯·ç¡®è®¤CSVä¸æ˜¯ç©ºæ–‡ä»¶ï¼Œä¸”ç¬¬ä¸€è¡ŒåŒ…å«è¡¨å¤´ï¼Œä¾‹å¦‚ï¼šid,category,content")

        if text_column not in df.columns:
            raise ValueError(f"åˆ— '{text_column}' ä¸å­˜åœ¨äºCSVæ–‡ä»¶ä¸­")

        texts = df[text_column].fillna("").tolist()
        embeddings = self.model.encode(texts, show_progress_bar=True)
        # å½’ä¸€åŒ–å‘é‡ï¼ˆL2èŒƒæ•°ï¼‰
        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

        # åˆ›å»ºæ–°ç´¢å¼•å¹¶æ·»åŠ å‘é‡
        self.index = NumpyVectorIndex(self.dimension)
        self.index.add(embeddings.astype('float32'))

        self.metadata = []
        for idx, row in df.iterrows():
            meta = {
                'id': idx,
                'text': row[text_column],
                'category': row[category_column] if category_column and category_column in df.columns else "æœªåˆ†ç±»",
                'timestamp': datetime.now().isoformat(),
                'other_fields': {k: v for k, v in row.items() if k not in [text_column, category_column]}
            }
            self.metadata.append(meta)

        self.save_index()
        return len(texts)

    def add_document(self, text, category="æœªåˆ†ç±»", **kwargs):
        """æ·»åŠ å•ä¸ªæ–‡æ¡£"""
        embedding = self.model.encode([text])
        embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)

        self.index.add(embedding.astype('float32'))

        meta = {
            'id': len(self.metadata),
            'text': text,
            'category': category,
            'timestamp': datetime.now().isoformat(),
            'other_fields': kwargs
        }
        self.metadata.append(meta)
        self.save_index()
        return meta['id']

    def delete_document(self, doc_id):
        """åˆ é™¤æ–‡æ¡£ï¼ˆé€šè¿‡é‡å»ºç´¢å¼•ï¼‰"""
        if doc_id < 0 or doc_id >= len(self.metadata):
            raise ValueError("æ— æ•ˆçš„æ–‡æ¡£ID")

        self.metadata.pop(doc_id)

        # æ›´æ–°å‰©ä½™æ–‡æ¡£çš„ID
        for i in range(doc_id, len(self.metadata)):
            self.metadata[i]['id'] = i

        # é‡å»ºç´¢å¼•
        if self.metadata:
            texts = [m['text'] for m in self.metadata]
            embeddings = self.model.encode(texts)
            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
            self.index = NumpyVectorIndex(self.dimension)
            self.index.add(embeddings.astype('float32'))
        else:
            self.index = NumpyVectorIndex(self.dimension)

        self.save_index()

    def update_document(self, doc_id, text=None, category=None, **kwargs):
        """æ›´æ–°æ–‡æ¡£"""
        if doc_id < 0 or doc_id >= len(self.metadata):
            raise ValueError("æ— æ•ˆçš„æ–‡æ¡£ID")

        if text:
            self.metadata[doc_id]['text'] = text
        if category:
            self.metadata[doc_id]['category'] = category
        if kwargs:
            self.metadata[doc_id]['other_fields'].update(kwargs)

        self.metadata[doc_id]['timestamp'] = datetime.now().isoformat()

        # é‡å»ºç´¢å¼•
        texts = [m['text'] for m in self.metadata]
        embeddings = self.model.encode(texts)
        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        self.index = NumpyVectorIndex(self.dimension)
        self.index.add(embeddings.astype('float32'))

        self.save_index()

    def search(self, query, top_k=Config.TOP_K):
        """è¯­ä¹‰æ£€ç´¢"""
        if len(self.index) == 0:
            return []

        query_embedding = self.model.encode([query])
        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)

        distances, indices = self.index.search(query_embedding.astype('float32'), min(top_k, len(self.index)))

        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(self.metadata):
                result = self.metadata[idx].copy()
                result['score'] = float(dist)
                results.append(result)

        return results

    def get_all_documents(self):
        """è·å–æ‰€æœ‰æ–‡æ¡£"""
        return self.metadata

    def get_statistics(self):
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        categories = {}
        for meta in self.metadata:
            cat = meta.get('category', 'æœªåˆ†ç±»')
            categories[cat] = categories.get(cat, 0) + 1

        return {
            'total_documents': len(self.metadata),
            'categories': categories,
            'index_size': len(self.index) if self.index else 0
        }


# ==================== RAGæ£€ç´¢å¢å¼ºç”Ÿæˆç±» ====================
# [RAGSystem ç±»ä¿æŒä¸å˜ï¼Œä¸åŸå§‹ä»£ç ç›¸åŒ]
class RAGSystem:
    def __init__(self, vector_db, llm_client=None):
        self.vector_db = vector_db
        self.llm_client = llm_client

    def set_llm_client(self, llm_client):
        """è®¾ç½®LLMå®¢æˆ·ç«¯"""
        self.llm_client = llm_client

    def rerank(self, query, results, top_k=Config.RERANK_TOP_K):
        """é‡æ’åºæ£€ç´¢ç»“æœ"""
        if not results:
            return []

        query_lower = query.lower()

        for result in results:
            text_lower = result['text'].lower()
            keyword_match = sum(1 for word in query_lower.split() if word in text_lower)
            result['rerank_score'] = result['score'] * 0.7 + (keyword_match / max(len(query_lower.split()), 1)) * 0.3

        results.sort(key=lambda x: x['rerank_score'], reverse=True)
        return results[:top_k]

    def generate_context(self, results):
        """ç”Ÿæˆä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ã€‚"

        context_parts = []
        for i, result in enumerate(results, 1):
            context_parts.append(
                f"[çŸ¥è¯†ç‰‡æ®µ{i}] ç±»åˆ«ï¼š{result['category']}\nå†…å®¹ï¼š{result['text']}\nç›¸å…³åº¦ï¼š{result.get('rerank_score', result['score']):.3f}")

        return "\n\n".join(context_parts)

    def generate_prompt(self, query, context):
        """ç”Ÿæˆæç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŸºäºé¢†åŸŸçŸ¥è¯†åº“çš„å†…å®¹å®‰å…¨å®¡æ ¸åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šè¿°çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®å¼•ç”¨çŸ¥è¯†åº“å†…å®¹
2. ä¿æŒå®¢è§‚ä¸­ç«‹
3. å¦‚æ¶‰åŠæ•æ„Ÿå†…å®¹ï¼Œéœ€è¦ç‰¹åˆ«è°¨æ…
4. è¯´æ˜åˆ¤æ–­ä¾æ®

å›ç­”ï¼š"""
        return prompt

    def answer(self, query, use_rerank=True, temperature=0.7):
        """å›ç­”ç”¨æˆ·é—®é¢˜"""
        # æ£€ç´¢
        results = self.vector_db.search(query, top_k=Config.TOP_K)

        if not results:
            return {
                'answer': "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚",
                'context': "",
                'retrieved_docs': []
            }

        # é‡æ’åº
        if use_rerank:
            results = self.rerank(query, results)
        else:
            results = results[:Config.RERANK_TOP_K]

        # ç”Ÿæˆä¸Šä¸‹æ–‡
        context = self.generate_context(results)

        # ç”Ÿæˆæç¤ºè¯
        prompt = self.generate_prompt(query, context)

        # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        if self.llm_client:
            answer = self.llm_client.generate(prompt, temperature=temperature)
        else:
            answer = "âš ï¸ æœªé…ç½®LLM APIï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆã€‚è¯·åœ¨ä¾§è¾¹æ é…ç½®APIå¯†é’¥ã€‚"

        return {
            'answer': answer,
            'context': context,
            'prompt': prompt,
            'retrieved_docs': results
        }

    def _generate_summary(self, results):
        """ç”Ÿæˆæ£€ç´¢ç»“æœæ‘˜è¦"""
        summary_parts = []
        for i, result in enumerate(results, 1):
            summary_parts.append(f"{i}. [{result['category']}] {result['text'][:100]}...")
        return "\n".join(summary_parts)


# ==================== Streamlit Webç•Œé¢ ====================
# [main å‡½æ•°ä¿æŒä¸å˜ï¼Œä¸åŸå§‹ä»£ç ç›¸åŒ]
def main():
    st.set_page_config(page_title="RAGå†…å®¹å®‰å…¨å®¡æ ¸ç³»ç»Ÿ", page_icon="ğŸ”", layout="wide")

    st.title("ğŸ” åŸºäºé¢†åŸŸçŸ¥è¯†åº“çš„å†…å®¹å®‰å…¨å®¡æ ¸ç³»ç»Ÿ")
    st.markdown("---")

    # åˆå§‹åŒ–
    if 'vector_db' not in st.session_state:
        st.session_state.vector_db = VectorDatabase()

    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = RAGSystem(st.session_state.vector_db)

    if 'llm_configured' not in st.session_state:
        st.session_state.llm_configured = False

    # ä¾§è¾¹æ  - ç³»ç»Ÿç®¡ç†
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")

        # LLMé…ç½®
        with st.expander("ğŸ¤– å¤§æ¨¡å‹é…ç½®", expanded=not st.session_state.llm_configured):
            selected_model = st.selectbox(
                "é€‰æ‹©æ¨¡å‹",
                list(Config.SUPPORTED_MODELS.keys())
            )

            model_info = Config.SUPPORTED_MODELS[selected_model]

            api_key = st.text_input(
                "APIå¯†é’¥",
                type="password",
                help=f"è¯·è¾“å…¥{selected_model}çš„APIå¯†é’¥"
            )

            col1, col2 = st.columns(2)
            with col1:
                temperature = st.slider("æ¸©åº¦å‚æ•°", 0.0, 1.0, 0.7, 0.1)
            with col2:
                if st.button("ä¿å­˜é…ç½®", type="primary"):
                    if api_key:
                        try:
                            llm_client = LLMClient(
                                provider=model_info["provider"],
                                model=model_info["model"],
                                api_key=api_key
                            )
                            st.session_state.rag_system.set_llm_client(llm_client)
                            st.session_state.llm_configured = True
                            st.session_state.temperature = temperature
                            st.success("âœ… é…ç½®æˆåŠŸï¼")
                        except Exception as e:
                            st.error(f"âŒ é…ç½®å¤±è´¥ï¼š{str(e)}")
                    else:
                        st.warning("âš ï¸ è¯·è¾“å…¥APIå¯†é’¥")

            if st.session_state.llm_configured:
                st.success(f"âœ… å½“å‰æ¨¡å‹ï¼š{selected_model}")

            # APIè·å–æŒ‡å—
            with st.expander("ğŸ“– APIå¯†é’¥è·å–æŒ‡å—"):
                st.markdown("""
                **OpenAI**
                - å®˜ç½‘ï¼šhttps://platform.openai.com/
                - æ³¨å†Œååœ¨API Keysé¡µé¢åˆ›å»º

                **é€šä¹‰åƒé—®**
                - å®˜ç½‘ï¼šhttps://dashscope.aliyun.com/
                - é˜¿é‡Œäº‘è´¦å·ç™»å½•åè·å–

                **æ™ºè°±AI**
                - å®˜ç½‘ï¼šhttps://open.bigmodel.cn/
                - æ³¨å†Œååœ¨ä¸ªäººä¸­å¿ƒè·å–
                """)

        st.markdown("---")
        st.header("ğŸ“Š çŸ¥è¯†åº“ç®¡ç†")

        tab1, tab2, tab3 = st.tabs(["æ„å»º", "ç®¡ç†", "ç»Ÿè®¡"])

        with tab1:
            st.subheader("ä»CSVæ„å»º")
            uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=['csv'])

            if uploaded_file:
                df = pd.read_csv(uploaded_file)
                st.write("é¢„è§ˆï¼š", df.head(3))

                text_col = st.selectbox("æ–‡æœ¬åˆ—", df.columns.tolist())
                category_col = st.selectbox("åˆ†ç±»åˆ—", ["æ— "] + df.columns.tolist())

                if st.button("æ„å»ºå‘é‡æ•°æ®åº“", type="primary"):
                    with st.spinner("æ„å»ºä¸­..."):
                        cat_col = None if category_col == "æ— " else category_col
                        count = st.session_state.vector_db.build_from_csv(
                            uploaded_file, text_col, cat_col
                        )
                        st.success(f"âœ… å¯¼å…¥ {count} æ¡æ•°æ®")
                        st.rerun()

        with tab2:
            st.subheader("æ·»åŠ çŸ¥è¯†")
            new_text = st.text_area("å†…å®¹", height=100)
            new_category = st.text_input("åˆ†ç±»", value="æœªåˆ†ç±»")

            if st.button("â• æ·»åŠ "):
                if new_text:
                    doc_id = st.session_state.vector_db.add_document(new_text, new_category)
                    st.success(f"âœ… ID: {doc_id}")
                    st.rerun()

            st.subheader("åˆ é™¤çŸ¥è¯†")
            del_id = st.number_input("æ–‡æ¡£ID", min_value=0, step=1)
            if st.button("ğŸ—‘ï¸ åˆ é™¤"):
                try:
                    st.session_state.vector_db.delete_document(del_id)
                    st.success("âœ… å·²åˆ é™¤")
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ {str(e)}")

        with tab3:
            stats = st.session_state.vector_db.get_statistics()
            st.metric("æ–‡æ¡£æ€»æ•°", stats['total_documents'])
            st.metric("ç´¢å¼•å¤§å°", stats['index_size'])

            st.subheader("åˆ†ç±»åˆ†å¸ƒ")
            if stats['categories']:
                for cat, count in stats['categories'].items():
                    st.write(f"- **{cat}**: {count}")

    # ä¸»ç•Œé¢ - RAGé—®ç­”
    col1, col2 = st.columns([1, 1])

    with col1:
        st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")

        if not st.session_state.llm_configured:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½®å¤§æ¨¡å‹API")

        query = st.text_area("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", height=120,
                             placeholder="ä¾‹å¦‚ï¼šå…³äºæŸå†å²äº‹ä»¶çš„å‡†ç¡®æè¿°æ˜¯ä»€ä¹ˆï¼Ÿ")

        col_a, col_b = st.columns(2)
        with col_a:
            use_rerank = st.checkbox("å¯ç”¨é‡æ’åº", value=True)
        with col_b:
            show_prompt = st.checkbox("æ˜¾ç¤ºæç¤ºè¯", value=False)

        if st.button("ğŸ” æäº¤æŸ¥è¯¢", type="primary", use_container_width=True):
            if query:
                if not st.session_state.llm_configured:
                    st.error("âŒ è¯·å…ˆé…ç½®å¤§æ¨¡å‹API")
                else:
                    with st.spinner("ğŸ¤” æ­£åœ¨æ€è€ƒ..."):
                        result = st.session_state.rag_system.answer(
                            query,
                            use_rerank,
                            temperature=st.session_state.get('temperature', 0.7)
                        )

                        st.subheader("ğŸ“ ç³»ç»Ÿå›ç­”")
                        st.markdown(result['answer'])

                        if show_prompt:
                            with st.expander("ğŸ“‹ æŸ¥çœ‹å®Œæ•´æç¤ºè¯"):
                                st.code(result['prompt'], language="text")

                        with st.expander("ğŸ” æŸ¥çœ‹æ£€ç´¢è¯¦æƒ…"):
                            st.markdown("**æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µï¼š**")
                            for i, doc in enumerate(result['retrieved_docs'], 1):
                                score = doc.get('rerank_score', doc['score'])
                                st.markdown(f"""
                                ---
                                **ç‰‡æ®µ {i}** | ç›¸å…³åº¦: `{score:.3f}`
                                - **ç±»åˆ«**: {doc['category']}
                                - **å†…å®¹**: {doc['text'][:200]}{'...' if len(doc['text']) > 200 else ''}
                                """)
            else:
                st.warning("âš ï¸ è¯·è¾“å…¥é—®é¢˜")

    with col2:
        st.header("ğŸ“š çŸ¥è¯†åº“æµè§ˆ")

        docs = st.session_state.vector_db.get_all_documents()

        if docs:
            search_term = st.text_input("ğŸ” æœç´¢")

            filtered_docs = docs
            if search_term:
                filtered_docs = [d for d in docs if search_term.lower() in d['text'].lower()]

            st.write(f"æ˜¾ç¤º {len(filtered_docs)} / {len(docs)} æ¡")

            for doc in filtered_docs[:20]:
                with st.expander(f"ID: {doc['id']} | {doc['category']}", expanded=False):
                    st.write(f"**å†…å®¹**: {doc['text']}")
                    st.caption(f"æ—¶é—´: {doc['timestamp']}")
                    if doc.get('other_fields'):
                        st.json(doc['other_fields'])
        else:
            st.info("ğŸ’¡ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆå¯¼å…¥æ•°æ®")


if __name__ == "__main__":
    main()